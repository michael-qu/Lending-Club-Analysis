{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f7aa3c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h1> Case Study 1 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import plotly.express as px # for USA map\n",
    "from mpl_toolkits import mplot3d # for 3D graph\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns # for heatmap\n",
    "from sklearn.model_selection import train_test_split # to split the dataset\n",
    "import statsmodels.api as sm # Linear Regression Approach 1\n",
    "from sklearn.linear_model import LinearRegression # Linear Regression Approach 2\n",
    "from sklearn.metrics import r2_score # to evaluate predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135ba6b",
   "metadata": {},
   "source": [
    "### 1. Dataset Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc8758",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('loans_full_schema.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00283cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4b6ba",
   "metadata": {},
   "source": [
    "#### 1.1 Remove NULL values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d596d",
   "metadata": {},
   "source": [
    "Let's see how many NULL values we have in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf66da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d21d4",
   "metadata": {},
   "source": [
    "Unfortunately there are columns where most of values are NULLs. They are completely useless, so just remove those columns where more than 1% of the rows for that column contain a null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df[[label for label in df if df[label].isnull().sum() <= 0.01 * df.shape[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a0632",
   "metadata": {},
   "source": [
    "Let's see how it looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a93136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c5f10",
   "metadata": {},
   "source": [
    "Ok, much better. Now we have to do something with those NULL values. We can:\n",
    "\n",
    "* remove rows cointain NULL values,\n",
    "* fill them with median or mode value,\n",
    "* or use some imputation and try to predict their missing values.\n",
    "\n",
    "Let's try the first option and see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.dropna()\n",
    "df_cleaned.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02fc38",
   "metadata": {},
   "source": [
    "It looks good, we removed less than 0.3% of rows. I think it's good enough and there's no point to do something more with that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb6e3d",
   "metadata": {},
   "source": [
    "#### 1.2 Remove useless columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5542309",
   "metadata": {},
   "source": [
    "Let's take a look on our cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5e41d",
   "metadata": {},
   "source": [
    "As we can see, \"issue_month\" only includes three values: \"Jan-2018\", \"Feb-2018\", and \"Mar-2018\". The time does not vary much and its impact to the applicant's credit profile is negligible. Thus column \"issue_month\" can be dropped.\n",
    "\n",
    "The same goes for \"disbursement_method\" which only includes two values: \"Cash\" and \"DirectPay\". The impact of these methods to the applicant's credit profile can also be neglected. Thus column \"disbursement_method\" can also be dropped.\n",
    "\n",
    "Since \"sub_grade\" (a grade assigned to the loan applicant based on his credit profile which determines the interest rate) has fully included the information of \"grade\", column \"grade\" can be dropped as well. A detailed explanation of subgrade can be found here: https://www.lendingclub.com/public/rates-and-fees.action\n",
    "\n",
    "Since the three major US credit bureaus no longer include tax liens on your credit reports, a tax lien is no longer able to affect your credit. Thus, column \"tax_liens\" can be dropped. Details can be found here: https://www.bankrate.com/finance/credit-cards/how-tax-liens-affect-your-credit-score/, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop([\"grade\", \"issue_month\", \"disbursement_method\", \"tax_liens\"], axis=1)\n",
    "df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46bb324",
   "metadata": {},
   "source": [
    "#### 1.3 Remove columns including values with insignificant frequencies and outlier rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab19f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in list(df_cleaned):\n",
    "    if len(df_cleaned[label].unique()) < 20:\n",
    "        print(df_cleaned[label].value_counts())\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5954807",
   "metadata": {},
   "source": [
    "We can see that features \"current_accounts_delinq\" and \"num_accounts_30d_past_due\" have only two possible values: 0 and 1, but with only 1 occurrences of 1 (less than 1%), so definitely they are insignificant.\n",
    "\n",
    "The same goes for \"num_collections_last_12m\" feature (non-zero values have only 1.3% frequency).\n",
    "\n",
    "Therefore we can drop these three columns.\n",
    "\n",
    "Since >1 values in \"num_historical_failed_to_pay\", >1 values in \"public_record_bankrupt\", >2 values in \"delinq_2y\", >6 values in \"num_mort_accounts\" and values other than \"Current\" and \"Fully Paid\" in \"loan_status\" have very low frequency (less than 2%), we can remove these outliers. The same goes with \"moving\", \"vacation\" and \"renewable energy\" values in \"loan_purpose\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop([\"current_accounts_delinq\", \"num_accounts_30d_past_due\", \"num_collections_last_12m\"], axis=1)\n",
    "df_cleaned = df_cleaned[df_cleaned['num_historical_failed_to_pay'] <= 2]\n",
    "df_cleaned = df_cleaned[df_cleaned['public_record_bankrupt'] <= 1]\n",
    "df_cleaned = df_cleaned[df_cleaned['delinq_2y'] <= 3]\n",
    "df_cleaned = df_cleaned[df_cleaned['num_mort_accounts'] <= 7]\n",
    "df_cleaned = df_cleaned[(df_cleaned['loan_status'] == \"Current\") | (df_cleaned['loan_status'] == \"Fully Paid\")]\n",
    "df_cleaned = df_cleaned[(df_cleaned['loan_purpose'] == \"debt_consolidation\") | (df_cleaned['loan_purpose'] == \"credit_card\")\n",
    "                       | (df_cleaned['loan_purpose'] == \"other\") | (df_cleaned['loan_purpose'] == \"home_improvement\")\n",
    "                       | (df_cleaned['loan_purpose'] == \"major_purchase\") | (df_cleaned['loan_purpose'] == \"medical\")\n",
    "                       | (df_cleaned['loan_purpose'] == \"house\") | (df_cleaned['loan_purpose'] == \"car\")\n",
    "                       | (df_cleaned['loan_purpose'] == \"small_business\") | (df_cleaned['loan_purpose'] == \"moving\")]\n",
    "df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8060aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582cf115",
   "metadata": {},
   "source": [
    "So the removed records are no more than 5%, which means the cleaned dataset can still represent the original dataset well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f598a3c",
   "metadata": {},
   "source": [
    "#### 1.4 Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044f540",
   "metadata": {},
   "source": [
    "To use any machine learning model we have to have only numerical data. So, let's do something with our non-numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.select_dtypes(include=[\"object\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c6f8c",
   "metadata": {},
   "source": [
    "Since each grade (A, B, C, D, E, F) and the associated subgrade have its own sequence, we can map them into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7002659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgrade_mapping(lst: list) -> dict:\n",
    "    mapping = {}\n",
    "    for subgrade in lst:\n",
    "        \n",
    "        subgrade = subgrade.strip()\n",
    "        letter = subgrade[0]\n",
    "        num = int(subgrade[1])\n",
    "        if letter == 'A':\n",
    "            m = 0\n",
    "        elif letter == 'B':\n",
    "            m = 1\n",
    "        elif letter == 'C':\n",
    "            m = 2\n",
    "        elif letter == 'D':\n",
    "            m = 3\n",
    "        elif letter == 'E':\n",
    "            m = 4\n",
    "        elif letter == 'F':\n",
    "            m = 5\n",
    "        elif letter == 'G':\n",
    "            m = 6\n",
    "        else:\n",
    "            m = np.nan\n",
    "        \n",
    "        mapping[subgrade] = m*5+num\n",
    "    return mapping\n",
    "\n",
    "df_cleaned[\"sub_grade\"] = df_cleaned[\"sub_grade\"].map(subgrade_mapping(df_cleaned[\"sub_grade\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b7fa7",
   "metadata": {},
   "source": [
    "To show the impact of states, let's plot the average interest rate by states on the US map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ac21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_by_state = df_cleaned.groupby('state').mean()\n",
    "df_avg_by_state = df_avg_by_state.reset_index()\n",
    "\n",
    "fig = px.choropleth(df_avg_by_state,  # Input Pandas DataFrame\n",
    "                    locations=\"state\",  # DataFrame column with locations\n",
    "                    color=\"interest_rate\",  # DataFrame column with color values\n",
    "                    hover_name=\"state\", # DataFrame column hover info\n",
    "                    locationmode = 'USA-states',# Set to plot as US States\n",
    "                    color_continuous_scale=\"purp\") #set color scale\n",
    "fig.update_layout(\n",
    "    title_text = 'Interest Rate by States', # Create a Title\n",
    "    geo_scope='usa',  # Plot only the USA instead of globe\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c3fb1",
   "metadata": {},
   "source": [
    "As we can see from the plot above, the average interest rate has no significant difference among different states, and there is also no significant trend on the map. Thus, we decide to drop this feature otherwise too many dummy indicators will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb17c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop([\"state\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222d891",
   "metadata": {},
   "source": [
    "The rest of the categorical features can be either mapped to integers, or to dummy indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5fac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cleaned[\"initial_list_status\"] = df_cleaned[\"initial_list_status\"].map({\"fractional\": 1, \"whole\": 0})\n",
    "df_cleaned[\"application_type\"] = df_cleaned[\"application_type\"].map({\"individual\": 1, \"joint\": 0})\n",
    "df_cleaned[\"loan_status\"] = df_cleaned[\"loan_status\"].map({\"Current\": 1, \"Fully Paid\": 0})\n",
    "df_cleaned[\"verified_income\"] = df_cleaned[\"verified_income\"].map({\"Not Verified\": 2, \"Source Verified\": 1, \"Verified\": 0})\n",
    "df_cleaned[\"homeownership\"] = df_cleaned[\"homeownership\"].map({\"RENT\": 2, \"MORTGAGE\": 1, \"OWN\": 0})\n",
    "df_cleaned = pd.get_dummies(df_cleaned, columns=list(df_cleaned.select_dtypes(include=[\"object\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a3fbd5",
   "metadata": {},
   "source": [
    "We can calculate the correlation matrix for the cleaned dataset, and then plot the values int the heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f84280",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = df_cleaned.corr()\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd6c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data using heatmap\n",
    "fig, ax = plt.subplots(figsize = (15,10))\n",
    "sns.heatmap(df_cleaned.corr(), cmap=\"YlGnBu\", annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca40ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C['interest_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78fce4",
   "metadata": {},
   "source": [
    "As we can see from the graph and table above, the most relevant variable is sub_grade (0.993) and paid_interest(0.522)\n",
    "We can plot them as scatter plots to see the data distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,4))\n",
    "ax.scatter(df_cleaned['sub_grade'], df_cleaned['interest_rate'], color='blue', alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('Subgrade', fontsize=12)\n",
    "ax.set_ylabel('Interest Rate (%)', fontsize=12)\n",
    "ax.set_title(\"Interest Rate by Subgrade\", size=14)\n",
    "# Set numerical y axis\n",
    "ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "# Set categorical ticks for x axis\n",
    "ax.set_xticks(np.arange(7)*5)\n",
    "ax.set_xticklabels(['A','B','C','D', 'E', 'F', 'G'], size=12)\n",
    "#ax.tick_params(axis='x', length=0)    #Hide ticks in x axis\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(.5)\n",
    "ax.spines['bottom'].set_linewidth(.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49737e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,4))\n",
    "ax.scatter(df_cleaned['paid_interest'], df_cleaned['interest_rate'], color='blue', alpha=0.1)\n",
    "\n",
    "ax.set_xlabel('Paid Interest', fontsize=12)\n",
    "ax.set_ylabel('Interest Rate (%)', fontsize=12)\n",
    "ax.set_title(\"Interest Rate by Paid Interest\", size=14)\n",
    "# Set numerical y axis\n",
    "ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(.5)\n",
    "ax.spines['bottom'].set_linewidth(.5)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef79469",
   "metadata": {},
   "source": [
    "We can even plot them into 3D graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating figure\n",
    "fig = plt.figure(figsize = (30, 10))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    " \n",
    "# Creating plot\n",
    "ax.scatter3D(df_cleaned['paid_interest'], df_cleaned['sub_grade'], df_cleaned['interest_rate'],\n",
    "             color = 'blue', s=3,alpha=0.3)\n",
    "\n",
    "# Set categorical ticks for y axis\n",
    "ax.set_yticks(np.arange(7)*5)\n",
    "ax.set_yticklabels(['A','B','C','D', 'E', 'F', 'G'], size=12)\n",
    "\n",
    "ax.set_xlabel('Paid Interest')\n",
    "ax.set_ylabel('Subgrade')\n",
    "ax.set_zlabel('Interest Rate (%)')\n",
    "plt.title(\"Interest Rate Distribution with Subgrade and Paid Interest\")\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7955ce2",
   "metadata": {},
   "source": [
    "As we can see from the graphs above, subgrade has a strong linear correlation to interest rate; paid interest mainly determine the lower limit of the interest rate, but has no restraint to the upper limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e21c63",
   "metadata": {},
   "source": [
    "### 2. Modeling Approach 1: Linear Regression Using `statsmodel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10897fe6",
   "metadata": {},
   "source": [
    "#### 2.1 Perform Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f13e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned['sub_grade']\n",
    "y = df_cleaned['interest_rate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7,\n",
    "                                                    test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "By default, the statsmodel library fits a line that passes through the origin.\n",
    "But if we observe the simple linear regression equation y = c + mX,\n",
    "it has an intercept value as c.\n",
    "So, to have an intercept, we need to add the add_constant attribute manually.\n",
    "'''\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "# Fitting the resgression line using Ordinary Least Square method\n",
    "lr = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# Printing the parameters\n",
    "lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3febe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a summary to list out all the different parameters of the regression line fitted\n",
    "lr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8160695",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3> Discussions: </h3>\n",
    "\n",
    "* The coefficient for `sub_grade` is 0.8457, and its corresponding p-value is very low (almost 0). That means the coefficient is statistically significant.\n",
    "* R-squared value is 0.986, which means that 98.6% of the interest rate variance can be explained by the subgrade column using this line.\n",
    "* Prob (F-statistic) has a very low p-value, practically zero, which gives us that the model fit is statistically significant.\n",
    "\n",
    "Since the fit is significant, let's go ahead and visualize how well the straight-line fits the scatter plot between `sub_grade` and `interest_rate` columns.\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ae5a8",
   "metadata": {},
   "source": [
    "#### 2.2 Visualization of the Regression Line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4352b",
   "metadata": {},
   "source": [
    "From the parameters shown above, we have obtained the values of the intercept and the slope of the straight line. The equation of the line is\n",
    "\n",
    "$interest$ $rate=3.8372+0.8457*subgrade $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e761fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the regression line\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.scatter(X_train, y_train)\n",
    "ax.plot(X_train, 3.8372 + 0.8457*X_train, 'r')\n",
    "ax.set_title('Best-fit regression line', fontsize = 15)\n",
    "ax.set_xlabel('Subgrade', fontsize = 12)\n",
    "ax.set_ylabel('Interest Rate(%)', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551a189",
   "metadata": {},
   "source": [
    "#### 2.3 Residual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe96262",
   "metadata": {},
   "source": [
    "One of the major assumptions of the linear regression model is the error terms are normally distributed.\n",
    "\n",
    "$Error=y-\\hat{y}$\n",
    "\n",
    "where $y$ is the actual y value, and $\\hat{y}$ is the predicted y value.\n",
    "\n",
    "Now from the dataset, we have to predict the y value from the training dataset of X using the predict attribute. After that, we’ll create the error terms(Residuals) from the predicted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff20bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting y_value using traingn data of X\n",
    "y_train_pred = lr.predict(X_train_sm)\n",
    "\n",
    "# Creating residuals from the y_train data and predicted y_data\n",
    "res = (y_train - y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99afab",
   "metadata": {},
   "source": [
    "Now, let’s plot the histogram of the residuals and see whether it looks like normal distribution or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57921ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the histogram using the residual values\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "n, bins, patches = ax.hist(res, bins=25)\n",
    "ax.set_title('Error Terms', fontsize = 15)\n",
    "ax.set_xlabel('y_train - y_train_pred', fontsize = 15)\n",
    "ax.set_xlim(-4, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f2cc2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As we can see, the residuals are following the normal distribution graph with a mean 0.\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a2c9b",
   "metadata": {},
   "source": [
    "Now, make sure that the residuals are not following any specific pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a82343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for any patterns in the residuals\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.scatter(X_train, res)\n",
    "ax.set_ylim(-4, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe2b02",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Since the Residuals follow a normal distribution and do not follow any specific pattern, we can use the linear regression model we have built to evaluate test data.\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a6949",
   "metadata": {},
   "source": [
    "#### 2.4. Predictions on the Test Data and Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af817279",
   "metadata": {},
   "source": [
    "Now that we have fitted the regression line on our train dataset, we can make some predictions to the test data. Similar to the training dataset, we have to `add_constant` to the test data and predict the y values using the `predict` attribute present in the `statsmodel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d267cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a constant to X_test\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "\n",
    "# Predicting the y values corresponding to X_test_sm\n",
    "y_test_pred = lr.predict(X_test_sm)\n",
    "\n",
    "# Printing the first 15 predicted values\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f8cb8",
   "metadata": {},
   "source": [
    "Now, let’s calculate the `R2` value for the above-predicted y-values. We can do that by merely importing the `r2_score` library from `sklearn.metrics` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the R-squared value\n",
    "r_squared = r2_score(y_test, y_test_pred)\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563d640",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Since the R2 value on training data is 0.986, the R2 value on test data is within 5% of the R2 value on training data. In this case we can conclude that the model is pretty stable. This means, what the model has learned on the trainign set can generalize on the unseen test set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def508d",
   "metadata": {},
   "source": [
    "Let's visualize the line on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.scatter(X_test, y_test)\n",
    "ax.plot(X_test, y_test_pred, 'r')\n",
    "ax.set_title('Best-fit regression line', fontsize = 15)\n",
    "ax.set_xlabel('Subgrade', fontsize = 12)\n",
    "ax.set_ylabel('Interest Rate(%)', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2c9e5",
   "metadata": {},
   "source": [
    "### 3. Modeling Approach 2: Linear Regression Using `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd3a2a",
   "metadata": {},
   "source": [
    "#### 3.1 Perform Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lm, X_test_lm, y_train_lm, y_test_lm = train_test_split(X, y, train_size = 0.7, \n",
    "                                                                test_size = 0.3, random_state = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984b75b",
   "metadata": {},
   "source": [
    "For simple linear regression, we need to add a column to perform the regression fit properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the train set without adding column\n",
    "print(\"The shape of X_train before adding a column is \", X_train_lm.shape)\n",
    "\n",
    "# Adding additional column to the train and test data\n",
    "X_train_lm = X_train_lm.values.reshape(-1,1)\n",
    "X_test_lm = X_test_lm.values.reshape(-1,1)\n",
    "\n",
    "print(\"The shape of X_train after adding a column is \", X_train_lm.shape)\n",
    "print(\"The shape of X_test after adding a column is \", X_test_lm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fd42c",
   "metadata": {},
   "source": [
    "Now we can conduct the linear regression using `sklearn.linear_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab005820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an object of Linear Regression\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fit the model using .fit() method\n",
    "lm.fit(X_train_lm, y_train_lm)\n",
    "\n",
    "# Intercept value\n",
    "print(\"Intercept :\",lm.intercept_)\n",
    "\n",
    "# Slope value\n",
    "print('Slope :',lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ff552",
   "metadata": {},
   "source": [
    "The straight-line equation we get for the above values is,\n",
    "\n",
    "$interest$ $rate=3.8372+0.8467*subgrade$\n",
    "\n",
    "If we observe, the equation we got here is the same as the one we got in the `statsmodel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd4cc53",
   "metadata": {},
   "source": [
    "#### 3.2 Predictions on the Test Data and Evaluations¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2903b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions of y_value\n",
    "y_train_pred = lm.predict(X_train_lm)\n",
    "y_test_pred = lm.predict(X_test_lm)\n",
    "\n",
    "# Comparing the r2 value of both train and test data\n",
    "print(r2_score(y_train,y_train_pred))\n",
    "print(r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42d056",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Same as the statesmodel, the R² value on test data is within 5% of the R² value on training data. We can apply the model to the unseen test set in the future.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ff0dc",
   "metadata": {},
   "source": [
    "### 4. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c26c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "* Data cleaning is performed to the original dataset. Unrelevant columns and outlier rows are removed.\n",
    "\n",
    "* After calculating the correlation coefficients, it is found that `sub_grade` is able to dominate the target variable `interest_rate`.\n",
    "    \n",
    "* In this case, we can simply use numerized `sub_grade` value to build up linear regression model in `statsmodel` and `sklearn`. Both of the model predictions have shown good agreement with the actual values.\n",
    "    \n",
    "* Model evaluations are made to both modeling approaches and r2 score is used to prove the model has enough robustness and stability.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9f55d",
   "metadata": {},
   "source": [
    "### 5. Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b399082",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/simple-linear-regression-model-using-python-machine-learning-eab7924d18b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c57ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
